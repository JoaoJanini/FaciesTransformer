{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heOekHXdFLvS"
      },
      "source": [
        "## Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNPKl6i8FJes",
        "outputId": "740d8ec0-66e4-4464-a0a1-57807aa47046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ai0mx9U9Zc"
      },
      "source": [
        "## Import repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEmL3M0EkbRn",
        "outputId": "540dc099-0eb4-40ae-94d1-92a90d59eaa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n",
            "Cloning into 'learning'...\n",
            "remote: Enumerating objects: 223, done.\u001b[K\n",
            "remote: Counting objects: 100% (223/223), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 223 (delta 108), reused 170 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (223/223), 241.15 KiB | 6.35 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n"
          ]
        }
      ],
      "source": [
        "%rm -rf /content/learning\n",
        "import getpass\n",
        "!git clone --branch seq2seq https://{getpass.getpass()}@github.com/JoaoJanini/learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1-8t1kQmMby"
      },
      "outputs": [],
      "source": [
        "!sleep 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHjmuJYhWoI5",
        "outputId": "9c39ffc2-6270-4791-9073-1fef23585244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/learning\n"
          ]
        }
      ],
      "source": [
        "%cd learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlH_RmEXhuUm"
      },
      "outputs": [],
      "source": [
        "%mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIAA6KzFWMpq",
        "outputId": "5b7d9eb7-e2ff-4867-f71b-7bb15f8bdb1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.50.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (2022.9.24)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->-r requirements.txt (line 5)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->-r requirements.txt (line 5)) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 9)) (1.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics->-r requirements.txt (line 10)) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics->-r requirements.txt (line 10)) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGHFjYn7W-Sp"
      },
      "source": [
        "# Sequence to Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvf1FcV4jC63"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "path = \"./seq2seq\"\n",
        "if path not in sys.path:\n",
        "  sys.path.append(path)\n",
        "else:\n",
        "  print(path + \"already in path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhe1k3RvjYlO",
        "outputId": "96cbc7e0-7861-4bd7-824f-56df1fb7405b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " './seq2seq']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "957DWfDKe61U"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/Coding/trained_models_sequence_2_label/base_model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA1Fy5OZ1oT1"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odiTVvrE1qMt"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "@author : Hyunwoong\n",
        "@when : 2019-10-22\n",
        "@homepage : https://github.com/gusdnd852\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "# GPU device setting\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# dataset settings\n",
        "SEQUENCE_LEN = 10\n",
        "TRAINING_RATIO = 0.8\n",
        "WIRELINE_LOGS_HEADER = [\"GR\", \"NPHI\"]\n",
        "LABEL_COLUMN_HEADER = [\"FORCE_2020_LITHOFACIES_LITHOLOGY\"]\n",
        "\n",
        "# model parameter setting\n",
        "batch_size = 640\n",
        "max_len = 256\n",
        "d_model = 512\n",
        "n_layers = 6\n",
        "n_heads = 8\n",
        "ffn_hidden = 2048\n",
        "drop_prob = 0.1\n",
        "\n",
        "# optimizer parameter setting\n",
        "init_lr = 1e-4\n",
        "factor = 0.9\n",
        "adam_eps = 5e-9\n",
        "patience = 10\n",
        "warmup = 100\n",
        "epoch = 10\n",
        "clip = 1.0\n",
        "weight_decay = 5e-4\n",
        "inf = float('inf')\n",
        "\n",
        "correct_on_train = []\n",
        "correct_on_test = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S501D4Xk1rSh",
        "outputId": "55b75a54-6423-4f97-cdca-8f8ed6b87ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data structure: [lines, timesteps, features]\n",
            "train data size: [(76492, 10, 2)]\n",
            "Number of classes: 12\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "@author : Hyunwoong\n",
        "@when : 2019-10-29\n",
        "@homepage : https://github.com/gusdnd852\n",
        "\"\"\"\n",
        "from torch.utils.data import DataLoader\n",
        "from seq2seq.dataset.dataset import WellsDataset\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "train_dataset = WellsDataset(dataset_type = \"train\",sequence_len = SEQUENCE_LEN, model_type=\"seq2seq\",feature_columns = WIRELINE_LOGS_HEADER, label_columns = LABEL_COLUMN_HEADER)\n",
        "test_dataset = WellsDataset(dataset_type = \"test\", sequence_len = SEQUENCE_LEN, model_type=\"seq2seq\",feature_columns = WIRELINE_LOGS_HEADER, label_columns = LABEL_COLUMN_HEADER, scaler = train_dataset.scaler, output_len= train_dataset.output_len)\n",
        "\n",
        "DATA_LEN = train_dataset.train_len \n",
        "d_input = train_dataset.input_len  \n",
        "d_channel = train_dataset.channel_len  \n",
        "d_output = train_dataset.output_len\n",
        "TRAIN_DATA_LEN = int(DATA_LEN * TRAINING_RATIO)\n",
        "train_dataset, validation_dataset = random_split(train_dataset, lengths=[TRAIN_DATA_LEN, DATA_LEN - TRAIN_DATA_LEN])\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "validation_loader = DataLoader(dataset=validation_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "print('data structure: [lines, timesteps, features]')\n",
        "print(f'train data size: [{DATA_LEN, d_input, d_channel}]')\n",
        "print(f'Number of classes: {d_output}')\n",
        "\n",
        "dec_voc_size = d_output\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ld2662I2lDq"
      },
      "source": [
        "### train_functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTuihd1x1_iT",
        "outputId": "ccfad0b5-3c6f-44c1-fe32-4c29ed167805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model has 52,091,662 trainable parameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "@author : Hyunwoong\n",
        "@when : 2019-10-22\n",
        "@homepage : https://github.com/gusdnd852\n",
        "\"\"\"\n",
        "import math\n",
        "import time\n",
        "from torch import nn, optim\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "from seq2seq.transformer.transformer import Transformer\n",
        "from seq2seq.util.epoch_timer import epoch_time\n",
        "\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.kaiming_uniform(m.weight.data)\n",
        "\n",
        "model = Transformer(d_model=d_model,\n",
        "                    d_channel=d_channel,\n",
        "                    d_input=d_input,\n",
        "                    dec_voc_size=dec_voc_size,\n",
        "                    max_len=max_len,\n",
        "                    ffn_hidden=ffn_hidden,\n",
        "                    n_head=n_heads,\n",
        "                    n_layers=n_layers,\n",
        "                    drop_prob=drop_prob,\n",
        "                    device=device\n",
        "                    ).to(device)\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "model.apply(initialize_weights)\n",
        "optimizer = Adam(params=model.parameters(),\n",
        "                 lr=init_lr,\n",
        "                 weight_decay=weight_decay,\n",
        "                 eps=adam_eps)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                                 verbose=True,\n",
        "                                                 factor=factor,\n",
        "                                                 patience=patience)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for i, (src, trg) in enumerate(iterator):\n",
        "        optimizer.zero_grad()\n",
        "        src = src.to(device)\n",
        "        trg = trg.to(device)\n",
        "        # Add <sos> token to the beginning of the target sentence\n",
        "        trg = torch.cat(((torch.zeros((trg.shape[0], 1)).to(device)), trg), dim=1).long()\n",
        "        output = model(src, trg[:, :-1])\n",
        "        \n",
        "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "        trg = trg[:, 1:].contiguous().view(-1)\n",
        "\n",
        "        loss = criterion(output_reshape, trg)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        print(\"step :\", round((i / len(iterator)) * 100, 2), \"% , loss :\", loss.item())\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    batch_bleu = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        accuracy = torchmetrics.Accuracy().to(device)\n",
        "        for i, (src, trg) in enumerate(iterator):\n",
        "            src = src.to(device)\n",
        "            trg = trg.to(device)\n",
        "            trg = torch.cat(((torch.zeros((trg.shape[0], 1)).to(device)), trg), dim=1).long()\n",
        "            output = model(src, trg[:, :-1])\n",
        "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "            trg_true = trg[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output_reshape, trg_true)\n",
        "            epoch_loss += loss.item()\n",
        "            _, label_index = torch.max(output_reshape.data, dim=-1)\n",
        "            acc = accuracy(label_index, trg_true)\n",
        "        acc = accuracy.compute()\n",
        "\n",
        "        accuracy.reset()\n",
        "    return epoch_loss / len(iterator), acc\n",
        "\n",
        "def run(total_epoch, best_loss):\n",
        "    train_losses, test_losses, bleus = [], [], []\n",
        "    for step in range(total_epoch):\n",
        "        start_time = time.time()\n",
        "        train_loss = train(model, train_loader, optimizer, criterion, clip)\n",
        "        test_loss, test_accuracy = evaluate(model, test_loader, criterion)\n",
        "        valid_loss, validation_accuracy = evaluate(model, validation_loader, criterion)\n",
        "        end_time = time.time()\n",
        "\n",
        "        if step > warmup:\n",
        "            scheduler.step(valid_loss)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(valid_loss)\n",
        "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            torch.save(model.state_dict(), model_path+'-{0}.pt'.format(valid_loss))\n",
        "\n",
        "        f = open(model_path+'-train_loss.txt', 'w')\n",
        "        f.write(str(train_losses))\n",
        "        f.close()\n",
        "\n",
        "        f = open(model_path+'-bleu.txt', 'w')\n",
        "        f.write(str(bleus))\n",
        "        f.close()\n",
        "\n",
        "        f = open(model_path+'-test_loss.txt', 'w')\n",
        "        f.write(str(test_losses))\n",
        "        f.close()\n",
        "\n",
        "        print(f'Epoch: {step + 1} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "        print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\tVal Loss: {valid_loss:.3f} |  Val PPL: {math.exp(valid_loss):7.3f}')\n",
        "        print(f\"Val Accuracy : {validation_accuracy}\")\n",
        "        print(f'\\tTest Loss: {test_loss:.3f} |  Val PPL: {math.exp(test_loss):7.3f}')\n",
        "        print(f\" Test Accuracy : {test_accuracy}\")\n",
        "        print(f'\\tBLEU Score: {valid_loss:.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34A5gDLH2ov_"
      },
      "source": [
        "### run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ozJik6Km2dBa",
        "outputId": "133c2178-eea0-4206-d7cb-55e9a8c3fa14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step : 0.0 % , loss : 3.7512083053588867\n",
            "step : 1.04 % , loss : 1.7347835302352905\n",
            "step : 2.08 % , loss : 2.5026466846466064\n",
            "step : 3.12 % , loss : 1.5871424674987793\n",
            "step : 4.17 % , loss : 1.5979477167129517\n",
            "step : 5.21 % , loss : 1.3139911890029907\n",
            "step : 6.25 % , loss : 1.4188411235809326\n",
            "step : 7.29 % , loss : 1.2371487617492676\n",
            "step : 8.33 % , loss : 1.2896125316619873\n",
            "step : 9.38 % , loss : 1.3630881309509277\n",
            "step : 10.42 % , loss : 1.197631597518921\n",
            "step : 11.46 % , loss : 1.2907967567443848\n",
            "step : 12.5 % , loss : 1.2295007705688477\n",
            "step : 13.54 % , loss : 1.232176423072815\n",
            "step : 14.58 % , loss : 1.2562744617462158\n",
            "step : 15.62 % , loss : 1.1833546161651611\n",
            "step : 16.67 % , loss : 1.1943281888961792\n",
            "step : 17.71 % , loss : 1.156787395477295\n",
            "step : 18.75 % , loss : 1.2389497756958008\n",
            "step : 19.79 % , loss : 1.2077631950378418\n",
            "step : 20.83 % , loss : 1.1845499277114868\n",
            "step : 21.88 % , loss : 1.1469180583953857\n",
            "step : 22.92 % , loss : 1.165919303894043\n",
            "step : 23.96 % , loss : 1.14860999584198\n",
            "step : 25.0 % , loss : 1.0639503002166748\n",
            "step : 26.04 % , loss : 1.1099570989608765\n",
            "step : 27.08 % , loss : 1.1189113855361938\n",
            "step : 28.12 % , loss : 1.133331298828125\n",
            "step : 29.17 % , loss : 1.0893006324768066\n",
            "step : 30.21 % , loss : 1.0436546802520752\n",
            "step : 31.25 % , loss : 1.1271497011184692\n",
            "step : 32.29 % , loss : 1.081514835357666\n",
            "step : 33.33 % , loss : 1.0992708206176758\n",
            "step : 34.38 % , loss : 1.0708212852478027\n",
            "step : 35.42 % , loss : 1.086394190788269\n",
            "step : 36.46 % , loss : 1.1127104759216309\n",
            "step : 37.5 % , loss : 1.0679376125335693\n",
            "step : 38.54 % , loss : 1.0761791467666626\n",
            "step : 39.58 % , loss : 1.1315104961395264\n",
            "step : 40.62 % , loss : 1.120038390159607\n",
            "step : 41.67 % , loss : 1.0872949361801147\n",
            "step : 42.71 % , loss : 1.0633643865585327\n",
            "step : 43.75 % , loss : 1.026160717010498\n",
            "step : 44.79 % , loss : 1.1021995544433594\n",
            "step : 45.83 % , loss : 0.9677680134773254\n",
            "step : 46.88 % , loss : 1.066445231437683\n",
            "step : 47.92 % , loss : 0.9548669457435608\n",
            "step : 48.96 % , loss : 1.1000866889953613\n",
            "step : 50.0 % , loss : 1.0088039636611938\n",
            "step : 51.04 % , loss : 0.9827060699462891\n",
            "step : 52.08 % , loss : 1.0772101879119873\n",
            "step : 53.12 % , loss : 1.0511201620101929\n",
            "step : 54.17 % , loss : 1.0786279439926147\n",
            "step : 55.21 % , loss : 1.0759265422821045\n",
            "step : 56.25 % , loss : 1.083325982093811\n",
            "step : 57.29 % , loss : 1.0224395990371704\n",
            "step : 58.33 % , loss : 1.1062952280044556\n",
            "step : 59.38 % , loss : 1.0791701078414917\n",
            "step : 60.42 % , loss : 1.1967954635620117\n",
            "step : 61.46 % , loss : 1.0153024196624756\n",
            "step : 62.5 % , loss : 1.0392729043960571\n",
            "step : 63.54 % , loss : 1.0691665410995483\n",
            "step : 64.58 % , loss : 1.06718111038208\n",
            "step : 65.62 % , loss : 1.1361443996429443\n",
            "step : 66.67 % , loss : 1.0347951650619507\n",
            "step : 67.71 % , loss : 1.0652309656143188\n",
            "step : 68.75 % , loss : 1.0201302766799927\n",
            "step : 69.79 % , loss : 1.0579992532730103\n",
            "step : 70.83 % , loss : 1.0499367713928223\n",
            "step : 71.88 % , loss : 1.0143264532089233\n",
            "step : 72.92 % , loss : 1.0860134363174438\n",
            "step : 73.96 % , loss : 1.0103886127471924\n",
            "step : 75.0 % , loss : 0.9839260578155518\n",
            "step : 76.04 % , loss : 1.1871113777160645\n",
            "step : 77.08 % , loss : 1.017590045928955\n",
            "step : 78.12 % , loss : 0.970744252204895\n",
            "step : 79.17 % , loss : 1.0552181005477905\n",
            "step : 80.21 % , loss : 1.0941520929336548\n",
            "step : 81.25 % , loss : 0.9925883412361145\n",
            "step : 82.29 % , loss : 1.0074114799499512\n",
            "step : 83.33 % , loss : 1.0946226119995117\n",
            "step : 84.38 % , loss : 1.0139403343200684\n",
            "step : 85.42 % , loss : 1.0463953018188477\n",
            "step : 86.46 % , loss : 0.9986777305603027\n",
            "step : 87.5 % , loss : 1.0085692405700684\n",
            "step : 88.54 % , loss : 1.0977975130081177\n",
            "step : 89.58 % , loss : 1.1250672340393066\n",
            "step : 90.62 % , loss : 0.9791468977928162\n",
            "step : 91.67 % , loss : 1.0381150245666504\n",
            "step : 92.71 % , loss : 0.9873412847518921\n",
            "step : 93.75 % , loss : 0.9980008006095886\n",
            "step : 94.79 % , loss : 1.0779320001602173\n",
            "step : 95.83 % , loss : 1.1087310314178467\n",
            "step : 96.88 % , loss : 1.0894898176193237\n",
            "step : 97.92 % , loss : 1.0113338232040405\n",
            "step : 98.96 % , loss : 1.0544310808181763\n",
            "Epoch: 1 | Time: 0m 49s\n",
            "\tTrain Loss: 1.156 | Train PPL:   3.178\n",
            "\tVal Loss: 1.026 |  Val PPL:   2.791\n",
            "Val Accuracy : 0.6593502759933472\n",
            "\tTest Loss: 0.950 |  Val PPL:   2.587\n",
            " Test Accuracy : 0.6819519400596619\n",
            "\tBLEU Score: 1.026\n",
            "step : 0.0 % , loss : 1.0471749305725098\n",
            "step : 1.04 % , loss : 1.092729091644287\n",
            "step : 2.08 % , loss : 1.0007737874984741\n",
            "step : 3.12 % , loss : 0.9906441569328308\n",
            "step : 4.17 % , loss : 1.043811559677124\n",
            "step : 5.21 % , loss : 1.036445140838623\n",
            "step : 6.25 % , loss : 0.9583624005317688\n",
            "step : 7.29 % , loss : 1.0431385040283203\n",
            "step : 8.33 % , loss : 1.039502739906311\n",
            "step : 9.38 % , loss : 1.0382661819458008\n",
            "step : 10.42 % , loss : 0.9666194319725037\n",
            "step : 11.46 % , loss : 0.985919713973999\n",
            "step : 12.5 % , loss : 1.0014604330062866\n",
            "step : 13.54 % , loss : 0.998151957988739\n",
            "step : 14.58 % , loss : 1.0087859630584717\n",
            "step : 15.62 % , loss : 1.0288543701171875\n",
            "step : 16.67 % , loss : 0.9917701482772827\n",
            "step : 17.71 % , loss : 1.0266337394714355\n",
            "step : 18.75 % , loss : 1.0304498672485352\n",
            "step : 19.79 % , loss : 1.001686453819275\n",
            "step : 20.83 % , loss : 1.0541231632232666\n",
            "step : 21.88 % , loss : 0.9885444641113281\n",
            "step : 22.92 % , loss : 1.0500147342681885\n",
            "step : 23.96 % , loss : 1.0690064430236816\n",
            "step : 25.0 % , loss : 1.0122579336166382\n",
            "step : 26.04 % , loss : 0.9773313999176025\n",
            "step : 27.08 % , loss : 1.0151793956756592\n",
            "step : 28.12 % , loss : 1.02376127243042\n",
            "step : 29.17 % , loss : 1.0012145042419434\n",
            "step : 30.21 % , loss : 0.9659444689750671\n",
            "step : 31.25 % , loss : 0.9970042705535889\n",
            "step : 32.29 % , loss : 0.9456087350845337\n",
            "step : 33.33 % , loss : 1.0438669919967651\n",
            "step : 34.38 % , loss : 1.051723837852478\n",
            "step : 35.42 % , loss : 1.035355567932129\n",
            "step : 36.46 % , loss : 0.9811093807220459\n",
            "step : 37.5 % , loss : 0.9853518605232239\n",
            "step : 38.54 % , loss : 1.025723934173584\n",
            "step : 39.58 % , loss : 1.0325711965560913\n",
            "step : 40.62 % , loss : 1.0743224620819092\n",
            "step : 41.67 % , loss : 1.0521399974822998\n",
            "step : 42.71 % , loss : 1.0189229249954224\n",
            "step : 43.75 % , loss : 1.0713750123977661\n",
            "step : 44.79 % , loss : 1.035754919052124\n",
            "step : 45.83 % , loss : 1.100443959236145\n",
            "step : 46.88 % , loss : 0.9899569153785706\n",
            "step : 47.92 % , loss : 0.9544396996498108\n",
            "step : 48.96 % , loss : 1.0308420658111572\n",
            "step : 50.0 % , loss : 0.9860026836395264\n",
            "step : 51.04 % , loss : 1.0038564205169678\n",
            "step : 52.08 % , loss : 1.0358095169067383\n",
            "step : 53.12 % , loss : 1.058708667755127\n",
            "step : 54.17 % , loss : 0.9801429510116577\n",
            "step : 55.21 % , loss : 0.9571636915206909\n",
            "step : 56.25 % , loss : 1.0354299545288086\n",
            "step : 57.29 % , loss : 0.9301465749740601\n",
            "step : 58.33 % , loss : 0.9795249104499817\n",
            "step : 59.38 % , loss : 1.0230424404144287\n",
            "step : 60.42 % , loss : 1.0545964241027832\n",
            "step : 61.46 % , loss : 1.0405255556106567\n",
            "step : 62.5 % , loss : 1.0710175037384033\n",
            "step : 63.54 % , loss : 1.0257713794708252\n",
            "step : 64.58 % , loss : 0.9757417440414429\n",
            "step : 65.62 % , loss : 1.0043588876724243\n",
            "step : 66.67 % , loss : 0.9892577528953552\n",
            "step : 67.71 % , loss : 1.0142687559127808\n",
            "step : 68.75 % , loss : 1.0579246282577515\n",
            "step : 69.79 % , loss : 0.9776126742362976\n",
            "step : 70.83 % , loss : 1.0667049884796143\n",
            "step : 71.88 % , loss : 1.024195909500122\n",
            "step : 72.92 % , loss : 0.9708633422851562\n",
            "step : 73.96 % , loss : 0.9686403870582581\n",
            "step : 75.0 % , loss : 0.9708837866783142\n",
            "step : 76.04 % , loss : 0.9048179388046265\n",
            "step : 77.08 % , loss : 0.9779700636863708\n",
            "step : 78.12 % , loss : 0.9564990401268005\n",
            "step : 79.17 % , loss : 1.0105103254318237\n",
            "step : 80.21 % , loss : 1.0451489686965942\n",
            "step : 81.25 % , loss : 0.9725088477134705\n",
            "step : 82.29 % , loss : 1.0462552309036255\n",
            "step : 83.33 % , loss : 0.943297266960144\n",
            "step : 84.38 % , loss : 0.970458984375\n",
            "step : 85.42 % , loss : 0.9630257487297058\n",
            "step : 86.46 % , loss : 1.0527410507202148\n",
            "step : 87.5 % , loss : 0.9866400957107544\n",
            "step : 88.54 % , loss : 1.0367896556854248\n",
            "step : 89.58 % , loss : 1.0537011623382568\n",
            "step : 90.62 % , loss : 0.9707577228546143\n",
            "step : 91.67 % , loss : 0.9843874573707581\n",
            "step : 92.71 % , loss : 0.8516177535057068\n",
            "step : 93.75 % , loss : 0.9098780155181885\n",
            "step : 94.79 % , loss : 0.758003294467926\n",
            "step : 95.83 % , loss : 1.1039310693740845\n",
            "step : 96.88 % , loss : 0.9030804634094238\n",
            "step : 97.92 % , loss : 0.900347888469696\n",
            "step : 98.96 % , loss : 1.0957452058792114\n",
            "Epoch: 2 | Time: 0m 49s\n",
            "\tTrain Loss: 1.005 | Train PPL:   2.733\n",
            "\tVal Loss: 0.809 |  Val PPL:   2.246\n",
            "Val Accuracy : 0.7569710612297058\n",
            "\tTest Loss: 0.751 |  Val PPL:   2.118\n",
            " Test Accuracy : 0.7572420239448547\n",
            "\tBLEU Score: 0.809\n",
            "step : 0.0 % , loss : 0.8445023894309998\n",
            "step : 1.04 % , loss : 1.0577428340911865\n",
            "step : 2.08 % , loss : 1.2365641593933105\n",
            "step : 3.12 % , loss : 0.8260735273361206\n",
            "step : 4.17 % , loss : 0.8312192559242249\n",
            "step : 5.21 % , loss : 0.8198981285095215\n",
            "step : 6.25 % , loss : 0.7105451822280884\n",
            "step : 7.29 % , loss : 0.8051659464836121\n",
            "step : 8.33 % , loss : 0.8661997318267822\n",
            "step : 9.38 % , loss : 0.797761857509613\n",
            "step : 10.42 % , loss : 0.6871479153633118\n",
            "step : 11.46 % , loss : 0.7345548272132874\n",
            "step : 12.5 % , loss : 0.6566426753997803\n",
            "step : 13.54 % , loss : 0.6338647603988647\n",
            "step : 14.58 % , loss : 0.5971772074699402\n",
            "step : 15.62 % , loss : 0.543306827545166\n",
            "step : 16.67 % , loss : 0.6011627316474915\n",
            "step : 17.71 % , loss : 0.5328136086463928\n",
            "step : 18.75 % , loss : 0.5388744473457336\n",
            "step : 19.79 % , loss : 0.5420801639556885\n",
            "step : 20.83 % , loss : 0.5170784592628479\n",
            "step : 21.88 % , loss : 0.46608391404151917\n",
            "step : 22.92 % , loss : 0.5083532929420471\n",
            "step : 23.96 % , loss : 0.537848949432373\n",
            "step : 25.0 % , loss : 0.4581589996814728\n",
            "step : 26.04 % , loss : 0.4700205624103546\n",
            "step : 27.08 % , loss : 0.43124890327453613\n",
            "step : 28.12 % , loss : 0.4704190194606781\n",
            "step : 29.17 % , loss : 0.5227168798446655\n",
            "step : 30.21 % , loss : 0.4432883560657501\n",
            "step : 31.25 % , loss : 0.5545331239700317\n",
            "step : 32.29 % , loss : 0.4317775368690491\n",
            "step : 33.33 % , loss : 0.5952486991882324\n",
            "step : 34.38 % , loss : 0.5252094864845276\n",
            "step : 35.42 % , loss : 0.4288608431816101\n",
            "step : 36.46 % , loss : 0.6611854434013367\n",
            "step : 37.5 % , loss : 0.6841431260108948\n",
            "step : 38.54 % , loss : 0.520054042339325\n",
            "step : 39.58 % , loss : 0.5025253891944885\n",
            "step : 40.62 % , loss : 0.516474187374115\n",
            "step : 41.67 % , loss : 0.4654293358325958\n",
            "step : 42.71 % , loss : 0.4618696868419647\n",
            "step : 43.75 % , loss : 0.5091971755027771\n",
            "step : 44.79 % , loss : 0.46202799677848816\n",
            "step : 45.83 % , loss : 0.42704546451568604\n",
            "step : 46.88 % , loss : 0.4623640775680542\n",
            "step : 47.92 % , loss : 0.44946515560150146\n",
            "step : 48.96 % , loss : 0.4540048837661743\n",
            "step : 50.0 % , loss : 0.4423070251941681\n",
            "step : 51.04 % , loss : 0.4395714998245239\n",
            "step : 52.08 % , loss : 0.47551199793815613\n",
            "step : 53.12 % , loss : 0.400127112865448\n",
            "step : 54.17 % , loss : 0.35504114627838135\n",
            "step : 55.21 % , loss : 0.36457329988479614\n",
            "step : 56.25 % , loss : 0.4102693200111389\n",
            "step : 57.29 % , loss : 0.36646348237991333\n",
            "step : 58.33 % , loss : 0.45485684275627136\n",
            "step : 59.38 % , loss : 0.37578749656677246\n",
            "step : 60.42 % , loss : 0.33408987522125244\n",
            "step : 61.46 % , loss : 0.3278317153453827\n",
            "step : 62.5 % , loss : 0.3876863420009613\n",
            "step : 63.54 % , loss : 0.3733280599117279\n",
            "step : 64.58 % , loss : 0.4299286901950836\n",
            "step : 65.62 % , loss : 0.32276004552841187\n",
            "step : 66.67 % , loss : 0.36227381229400635\n",
            "step : 67.71 % , loss : 0.3013874292373657\n",
            "step : 68.75 % , loss : 0.3308979272842407\n",
            "step : 69.79 % , loss : 0.36501362919807434\n",
            "step : 70.83 % , loss : 0.332209050655365\n",
            "step : 71.88 % , loss : 0.4659402370452881\n",
            "step : 72.92 % , loss : 0.49843624234199524\n",
            "step : 73.96 % , loss : 0.38091468811035156\n",
            "step : 75.0 % , loss : 0.4622883200645447\n",
            "step : 76.04 % , loss : 0.3896770179271698\n",
            "step : 77.08 % , loss : 0.31370818614959717\n",
            "step : 78.12 % , loss : 0.31787657737731934\n",
            "step : 79.17 % , loss : 0.33257171511650085\n",
            "step : 80.21 % , loss : 0.34395113587379456\n",
            "step : 81.25 % , loss : 0.3026919662952423\n",
            "step : 82.29 % , loss : 0.32062971591949463\n",
            "step : 83.33 % , loss : 0.32286348938941956\n",
            "step : 84.38 % , loss : 0.293178528547287\n",
            "step : 85.42 % , loss : 0.3275206387042999\n",
            "step : 86.46 % , loss : 0.3110770285129547\n",
            "step : 87.5 % , loss : 0.3131450414657593\n",
            "step : 88.54 % , loss : 0.290418803691864\n",
            "step : 89.58 % , loss : 0.2755713164806366\n",
            "step : 90.62 % , loss : 0.24969176948070526\n",
            "step : 91.67 % , loss : 0.25174441933631897\n",
            "step : 92.71 % , loss : 0.25358647108078003\n",
            "step : 93.75 % , loss : 0.31699809432029724\n",
            "step : 94.79 % , loss : 0.2879874110221863\n",
            "step : 95.83 % , loss : 0.25594469904899597\n",
            "step : 96.88 % , loss : 0.28339579701423645\n",
            "step : 97.92 % , loss : 0.272438645362854\n",
            "step : 98.96 % , loss : 0.3165842890739441\n",
            "Epoch: 3 | Time: 0m 49s\n",
            "\tTrain Loss: 0.474 | Train PPL:   1.606\n",
            "\tVal Loss: 0.232 |  Val PPL:   1.261\n",
            "Val Accuracy : 0.9171645045280457\n",
            "\tTest Loss: 0.206 |  Val PPL:   1.229\n",
            " Test Accuracy : 0.9276315569877625\n",
            "\tBLEU Score: 0.232\n",
            "step : 0.0 % , loss : 0.2948610782623291\n",
            "step : 1.04 % , loss : 0.3137449026107788\n",
            "step : 2.08 % , loss : 0.2716028690338135\n",
            "step : 3.12 % , loss : 0.283976286649704\n",
            "step : 4.17 % , loss : 0.3180699050426483\n",
            "step : 5.21 % , loss : 0.2522145211696625\n",
            "step : 6.25 % , loss : 0.402094304561615\n",
            "step : 7.29 % , loss : 0.39129123091697693\n",
            "step : 8.33 % , loss : 0.290515661239624\n",
            "step : 9.38 % , loss : 0.28643208742141724\n",
            "step : 10.42 % , loss : 0.2792002558708191\n",
            "step : 11.46 % , loss : 0.2744911015033722\n",
            "step : 12.5 % , loss : 0.2786579430103302\n",
            "step : 13.54 % , loss : 0.2838786542415619\n",
            "step : 14.58 % , loss : 0.28768986463546753\n",
            "step : 15.62 % , loss : 0.2654353082180023\n",
            "step : 16.67 % , loss : 0.2865392565727234\n",
            "step : 17.71 % , loss : 0.2619921565055847\n",
            "step : 18.75 % , loss : 0.27379751205444336\n",
            "step : 19.79 % , loss : 0.2266928255558014\n",
            "step : 20.83 % , loss : 0.24291297793388367\n",
            "step : 21.88 % , loss : 0.25430744886398315\n",
            "step : 22.92 % , loss : 0.2598331570625305\n",
            "step : 23.96 % , loss : 0.2873423993587494\n",
            "step : 25.0 % , loss : 0.27217572927474976\n",
            "step : 26.04 % , loss : 0.2572173774242401\n",
            "step : 27.08 % , loss : 0.2668933570384979\n",
            "step : 28.12 % , loss : 0.26403701305389404\n",
            "step : 29.17 % , loss : 0.22914670407772064\n",
            "step : 30.21 % , loss : 0.25473904609680176\n",
            "step : 31.25 % , loss : 0.242223858833313\n",
            "step : 32.29 % , loss : 0.23849773406982422\n",
            "step : 33.33 % , loss : 0.27248337864875793\n",
            "step : 34.38 % , loss : 0.26867061853408813\n",
            "step : 35.42 % , loss : 0.2519333064556122\n",
            "step : 36.46 % , loss : 0.23146846890449524\n",
            "step : 37.5 % , loss : 0.2386741042137146\n",
            "step : 38.54 % , loss : 0.23442533612251282\n",
            "step : 39.58 % , loss : 0.24610786139965057\n",
            "step : 40.62 % , loss : 0.22274187207221985\n",
            "step : 41.67 % , loss : 0.24692943692207336\n",
            "step : 42.71 % , loss : 0.24343110620975494\n",
            "step : 43.75 % , loss : 0.25379297137260437\n",
            "step : 44.79 % , loss : 0.2625017464160919\n",
            "step : 45.83 % , loss : 0.23478135466575623\n",
            "step : 46.88 % , loss : 0.25770217180252075\n",
            "step : 47.92 % , loss : 0.21717780828475952\n",
            "step : 48.96 % , loss : 0.20573586225509644\n",
            "step : 50.0 % , loss : 0.24705776572227478\n",
            "step : 51.04 % , loss : 0.22482189536094666\n",
            "step : 52.08 % , loss : 0.20356860756874084\n",
            "step : 53.12 % , loss : 0.2374337762594223\n",
            "step : 54.17 % , loss : 0.2349795699119568\n",
            "step : 55.21 % , loss : 0.2260136604309082\n",
            "step : 56.25 % , loss : 0.18781571090221405\n",
            "step : 57.29 % , loss : 0.23165977001190186\n",
            "step : 58.33 % , loss : 0.20845608413219452\n",
            "step : 59.38 % , loss : 0.2992684245109558\n",
            "step : 60.42 % , loss : 0.2994159460067749\n",
            "step : 61.46 % , loss : 0.24539782106876373\n",
            "step : 62.5 % , loss : 0.2680108845233917\n",
            "step : 63.54 % , loss : 0.2657122313976288\n",
            "step : 64.58 % , loss : 0.2588456869125366\n",
            "step : 65.62 % , loss : 0.2706347405910492\n",
            "step : 66.67 % , loss : 0.23044733703136444\n",
            "step : 67.71 % , loss : 0.21926268935203552\n",
            "step : 68.75 % , loss : 0.18292663991451263\n",
            "step : 69.79 % , loss : 0.23497065901756287\n",
            "step : 70.83 % , loss : 0.22251790761947632\n",
            "step : 71.88 % , loss : 0.21117454767227173\n",
            "step : 72.92 % , loss : 0.2602771818637848\n",
            "step : 73.96 % , loss : 0.23401480913162231\n",
            "step : 75.0 % , loss : 0.21667610108852386\n",
            "step : 76.04 % , loss : 0.25563952326774597\n",
            "step : 77.08 % , loss : 0.26906290650367737\n",
            "step : 78.12 % , loss : 0.20146436989307404\n",
            "step : 79.17 % , loss : 0.22155684232711792\n",
            "step : 80.21 % , loss : 0.19848880171775818\n",
            "step : 81.25 % , loss : 0.22269219160079956\n",
            "step : 82.29 % , loss : 0.23797914385795593\n",
            "step : 83.33 % , loss : 0.21871642768383026\n",
            "step : 84.38 % , loss : 0.22019298374652863\n",
            "step : 85.42 % , loss : 0.22316555678844452\n",
            "step : 86.46 % , loss : 0.2194044440984726\n",
            "step : 87.5 % , loss : 0.2080923467874527\n",
            "step : 88.54 % , loss : 0.2434084713459015\n",
            "step : 89.58 % , loss : 0.19678829610347748\n",
            "step : 90.62 % , loss : 0.22503775358200073\n",
            "step : 91.67 % , loss : 0.23322147130966187\n",
            "step : 92.71 % , loss : 0.2363896369934082\n",
            "step : 93.75 % , loss : 0.205088272690773\n",
            "step : 94.79 % , loss : 0.23150049149990082\n",
            "step : 95.83 % , loss : 0.2155565619468689\n",
            "step : 96.88 % , loss : 0.24436067044734955\n",
            "step : 97.92 % , loss : 0.2319476306438446\n",
            "step : 98.96 % , loss : 0.1928078681230545\n",
            "Epoch: 4 | Time: 0m 49s\n",
            "\tTrain Loss: 0.248 | Train PPL:   1.282\n",
            "\tVal Loss: 0.186 |  Val PPL:   1.204\n",
            "Val Accuracy : 0.9246813654899597\n",
            "\tTest Loss: 0.155 |  Val PPL:   1.168\n",
            " Test Accuracy : 0.941328227519989\n",
            "\tBLEU Score: 0.186\n",
            "step : 0.0 % , loss : 0.2402256727218628\n",
            "step : 1.04 % , loss : 0.2011469006538391\n",
            "step : 2.08 % , loss : 0.2027105838060379\n",
            "step : 3.12 % , loss : 0.23294293880462646\n",
            "step : 4.17 % , loss : 0.23980925977230072\n",
            "step : 5.21 % , loss : 0.22507748007774353\n",
            "step : 6.25 % , loss : 0.23574917018413544\n",
            "step : 7.29 % , loss : 0.21221040189266205\n",
            "step : 8.33 % , loss : 0.26814836263656616\n",
            "step : 9.38 % , loss : 0.23370963335037231\n",
            "step : 10.42 % , loss : 0.2070103883743286\n",
            "step : 11.46 % , loss : 0.20188407599925995\n",
            "step : 12.5 % , loss : 0.22565904259681702\n",
            "step : 13.54 % , loss : 0.24096627533435822\n",
            "step : 14.58 % , loss : 0.22893953323364258\n",
            "step : 15.62 % , loss : 0.24523517489433289\n",
            "step : 16.67 % , loss : 0.23295646905899048\n",
            "step : 17.71 % , loss : 0.21547351777553558\n",
            "step : 18.75 % , loss : 0.23125259578227997\n",
            "step : 19.79 % , loss : 0.21827855706214905\n",
            "step : 20.83 % , loss : 0.1942451447248459\n",
            "step : 21.88 % , loss : 0.19510923326015472\n",
            "step : 22.92 % , loss : 0.19227197766304016\n",
            "step : 23.96 % , loss : 0.20749451220035553\n",
            "step : 25.0 % , loss : 0.18508705496788025\n",
            "step : 26.04 % , loss : 0.18461874127388\n",
            "step : 27.08 % , loss : 0.15805821120738983\n",
            "step : 28.12 % , loss : 0.17871280014514923\n",
            "step : 29.17 % , loss : 0.18432793021202087\n",
            "step : 30.21 % , loss : 0.20567095279693604\n",
            "step : 31.25 % , loss : 0.18664811551570892\n",
            "step : 32.29 % , loss : 0.17304034531116486\n",
            "step : 33.33 % , loss : 0.16832387447357178\n",
            "step : 34.38 % , loss : 0.204949289560318\n",
            "step : 35.42 % , loss : 0.2111915796995163\n",
            "step : 36.46 % , loss : 0.20909209549427032\n",
            "step : 37.5 % , loss : 0.19902116060256958\n",
            "step : 38.54 % , loss : 0.21846666932106018\n",
            "step : 39.58 % , loss : 0.18101930618286133\n",
            "step : 40.62 % , loss : 0.22993279993534088\n",
            "step : 41.67 % , loss : 0.23180371522903442\n",
            "step : 42.71 % , loss : 0.17293748259544373\n",
            "step : 43.75 % , loss : 0.2178083062171936\n",
            "step : 44.79 % , loss : 0.2400483936071396\n",
            "step : 45.83 % , loss : 0.21004034578800201\n",
            "step : 46.88 % , loss : 0.194038525223732\n",
            "step : 47.92 % , loss : 0.21279969811439514\n",
            "step : 48.96 % , loss : 0.19666795432567596\n",
            "step : 50.0 % , loss : 0.20740406215190887\n",
            "step : 51.04 % , loss : 0.23175948858261108\n",
            "step : 52.08 % , loss : 0.21111057698726654\n",
            "step : 53.12 % , loss : 0.19527477025985718\n",
            "step : 54.17 % , loss : 0.22646023333072662\n",
            "step : 55.21 % , loss : 0.16623501479625702\n",
            "step : 56.25 % , loss : 0.20423537492752075\n",
            "step : 57.29 % , loss : 0.20126484334468842\n",
            "step : 58.33 % , loss : 0.17244380712509155\n",
            "step : 59.38 % , loss : 0.177490696310997\n",
            "step : 60.42 % , loss : 0.1860814094543457\n",
            "step : 61.46 % , loss : 0.21680572628974915\n",
            "step : 62.5 % , loss : 0.19300179183483124\n",
            "step : 63.54 % , loss : 0.2008749395608902\n",
            "step : 64.58 % , loss : 0.17681054770946503\n",
            "step : 65.62 % , loss : 0.16757386922836304\n",
            "step : 66.67 % , loss : 0.16515474021434784\n",
            "step : 67.71 % , loss : 0.1627316027879715\n",
            "step : 68.75 % , loss : 0.22118334472179413\n",
            "step : 69.79 % , loss : 0.18868805468082428\n",
            "step : 70.83 % , loss : 0.2036173939704895\n",
            "step : 71.88 % , loss : 0.19109921157360077\n",
            "step : 72.92 % , loss : 0.18395809829235077\n",
            "step : 73.96 % , loss : 0.16603036224842072\n",
            "step : 75.0 % , loss : 0.16497144103050232\n",
            "step : 76.04 % , loss : 0.19350320100784302\n",
            "step : 77.08 % , loss : 0.19693292677402496\n",
            "step : 78.12 % , loss : 0.17556144297122955\n",
            "step : 79.17 % , loss : 0.16780641674995422\n",
            "step : 80.21 % , loss : 0.17628037929534912\n",
            "step : 81.25 % , loss : 0.140933558344841\n",
            "step : 82.29 % , loss : 0.15574252605438232\n",
            "step : 83.33 % , loss : 0.16551877558231354\n",
            "step : 84.38 % , loss : 0.1643206924200058\n",
            "step : 85.42 % , loss : 0.16374610364437103\n",
            "step : 86.46 % , loss : 0.16839458048343658\n",
            "step : 87.5 % , loss : 0.136896014213562\n",
            "step : 88.54 % , loss : 0.16358888149261475\n",
            "step : 89.58 % , loss : 0.1799004226922989\n",
            "step : 90.62 % , loss : 0.13002686202526093\n",
            "step : 91.67 % , loss : 0.11970660835504532\n",
            "step : 92.71 % , loss : 0.14518608152866364\n",
            "step : 93.75 % , loss : 0.12860175967216492\n",
            "step : 94.79 % , loss : 0.10823109745979309\n",
            "step : 95.83 % , loss : 0.13420851528644562\n",
            "step : 96.88 % , loss : 0.142293781042099\n",
            "step : 97.92 % , loss : 0.11563622206449509\n",
            "step : 98.96 % , loss : 0.09867130219936371\n",
            "Epoch: 5 | Time: 0m 49s\n",
            "\tTrain Loss: 0.191 | Train PPL:   1.210\n",
            "\tVal Loss: 0.090 |  Val PPL:   1.094\n",
            "Val Accuracy : 0.9714556336402893\n",
            "\tTest Loss: 0.084 |  Val PPL:   1.088\n",
            " Test Accuracy : 0.9736531376838684\n",
            "\tBLEU Score: 0.090\n",
            "step : 0.0 % , loss : 0.09077019989490509\n",
            "step : 1.04 % , loss : 0.1276675909757614\n",
            "step : 2.08 % , loss : 0.09865865856409073\n",
            "step : 3.12 % , loss : 0.10104840248823166\n",
            "step : 4.17 % , loss : 0.10378710925579071\n",
            "step : 5.21 % , loss : 0.08552056550979614\n",
            "step : 6.25 % , loss : 0.11745583266019821\n",
            "step : 7.29 % , loss : 0.08077242225408554\n",
            "step : 8.33 % , loss : 0.09525655955076218\n",
            "step : 9.38 % , loss : 0.09405361115932465\n",
            "step : 10.42 % , loss : 0.09629140049219131\n",
            "step : 11.46 % , loss : 0.06449314206838608\n",
            "step : 12.5 % , loss : 0.0859440416097641\n",
            "step : 13.54 % , loss : 0.07963455468416214\n",
            "step : 14.58 % , loss : 0.0591583251953125\n",
            "step : 15.62 % , loss : 0.058722104877233505\n",
            "step : 16.67 % , loss : 0.072596475481987\n",
            "step : 17.71 % , loss : 0.0709514245390892\n",
            "step : 18.75 % , loss : 0.05712136626243591\n",
            "step : 19.79 % , loss : 0.08763770014047623\n",
            "step : 20.83 % , loss : 0.06255349516868591\n",
            "step : 21.88 % , loss : 0.07274385541677475\n",
            "step : 22.92 % , loss : 0.04434980824589729\n",
            "step : 23.96 % , loss : 0.037014421075582504\n",
            "step : 25.0 % , loss : 0.045600250363349915\n",
            "step : 26.04 % , loss : 0.0519087128341198\n",
            "step : 27.08 % , loss : 0.03744696080684662\n",
            "step : 28.12 % , loss : 0.061352454125881195\n",
            "step : 29.17 % , loss : 0.04538777470588684\n",
            "step : 30.21 % , loss : 0.03584285452961922\n",
            "step : 31.25 % , loss : 0.04493850842118263\n",
            "step : 32.29 % , loss : 0.021449299529194832\n",
            "step : 33.33 % , loss : 0.038155291229486465\n",
            "step : 34.38 % , loss : 0.03544925898313522\n",
            "step : 35.42 % , loss : 0.022302906960248947\n",
            "step : 36.46 % , loss : 0.039943233132362366\n",
            "step : 37.5 % , loss : 0.03474732115864754\n",
            "step : 38.54 % , loss : 0.035064201802015305\n",
            "step : 39.58 % , loss : 0.0322302021086216\n",
            "step : 40.62 % , loss : 0.015331501141190529\n",
            "step : 41.67 % , loss : 0.037631310522556305\n",
            "step : 42.71 % , loss : 0.041475359350442886\n",
            "step : 43.75 % , loss : 0.02787051908671856\n",
            "step : 44.79 % , loss : 0.03532833233475685\n",
            "step : 45.83 % , loss : 0.03202937915921211\n",
            "step : 46.88 % , loss : 0.02685343287885189\n",
            "step : 47.92 % , loss : 0.02484697662293911\n",
            "step : 48.96 % , loss : 0.029249241575598717\n",
            "step : 50.0 % , loss : 0.016095098108053207\n",
            "step : 51.04 % , loss : 0.01740896888077259\n",
            "step : 52.08 % , loss : 0.018978875130414963\n",
            "step : 53.12 % , loss : 0.027846163138747215\n",
            "step : 54.17 % , loss : 0.017701108008623123\n",
            "step : 55.21 % , loss : 0.014501900412142277\n",
            "step : 56.25 % , loss : 0.014310376718640327\n",
            "step : 57.29 % , loss : 0.016636190935969353\n",
            "step : 58.33 % , loss : 0.037511974573135376\n",
            "step : 59.38 % , loss : 0.015135846100747585\n",
            "step : 60.42 % , loss : 0.021136987954378128\n",
            "step : 61.46 % , loss : 0.020024722442030907\n",
            "step : 62.5 % , loss : 0.033373646438121796\n",
            "step : 63.54 % , loss : 0.01779739186167717\n",
            "step : 64.58 % , loss : 0.016720842570066452\n",
            "step : 65.62 % , loss : 0.017610419541597366\n",
            "step : 66.67 % , loss : 0.028727930039167404\n",
            "step : 67.71 % , loss : 0.031429726630449295\n",
            "step : 68.75 % , loss : 0.027516920119524002\n",
            "step : 69.79 % , loss : 0.016957242041826248\n",
            "step : 70.83 % , loss : 0.022625302895903587\n",
            "step : 71.88 % , loss : 0.03096601739525795\n",
            "step : 72.92 % , loss : 0.025247223675251007\n",
            "step : 73.96 % , loss : 0.013504421338438988\n",
            "step : 75.0 % , loss : 0.023647615686058998\n",
            "step : 76.04 % , loss : 0.009771455079317093\n",
            "step : 77.08 % , loss : 0.028547413647174835\n",
            "step : 78.12 % , loss : 0.007444866932928562\n",
            "step : 79.17 % , loss : 0.019252030178904533\n",
            "step : 80.21 % , loss : 0.01465412974357605\n",
            "step : 81.25 % , loss : 0.013672917149960995\n",
            "step : 82.29 % , loss : 0.01459803618490696\n",
            "step : 83.33 % , loss : 0.009713484905660152\n",
            "step : 84.38 % , loss : 0.013049457222223282\n",
            "step : 85.42 % , loss : 0.006876803003251553\n",
            "step : 86.46 % , loss : 0.016677893698215485\n",
            "step : 87.5 % , loss : 0.011355211958289146\n",
            "step : 88.54 % , loss : 0.009950756095349789\n",
            "step : 89.58 % , loss : 0.018177954480051994\n",
            "step : 90.62 % , loss : 0.01493604201823473\n",
            "step : 91.67 % , loss : 0.011000118218362331\n",
            "step : 92.71 % , loss : 0.013198990374803543\n",
            "step : 93.75 % , loss : 0.012289101257920265\n",
            "step : 94.79 % , loss : 0.007541124243289232\n",
            "step : 95.83 % , loss : 0.0200439915060997\n",
            "step : 96.88 % , loss : 0.02105277217924595\n",
            "step : 97.92 % , loss : 0.00795244425535202\n",
            "step : 98.96 % , loss : 0.016619037836790085\n",
            "Epoch: 6 | Time: 0m 49s\n",
            "\tTrain Loss: 0.038 | Train PPL:   1.039\n",
            "\tVal Loss: 0.004 |  Val PPL:   1.005\n",
            "Val Accuracy : 0.9988038539886475\n",
            "\tTest Loss: 0.006 |  Val PPL:   1.006\n",
            " Test Accuracy : 0.9983112215995789\n",
            "\tBLEU Score: 0.004\n",
            "step : 0.0 % , loss : 0.013367600739002228\n",
            "step : 1.04 % , loss : 0.01872461661696434\n",
            "step : 2.08 % , loss : 0.013533000834286213\n",
            "step : 3.12 % , loss : 0.019007975235581398\n",
            "step : 4.17 % , loss : 0.01802036352455616\n",
            "step : 5.21 % , loss : 0.008757206611335278\n",
            "step : 6.25 % , loss : 0.030423251911997795\n",
            "step : 7.29 % , loss : 0.022173084318637848\n",
            "step : 8.33 % , loss : 0.006808803882449865\n",
            "step : 9.38 % , loss : 0.010074501857161522\n",
            "step : 10.42 % , loss : 0.014309952035546303\n",
            "step : 11.46 % , loss : 0.009102484211325645\n",
            "step : 12.5 % , loss : 0.01484995149075985\n",
            "step : 13.54 % , loss : 0.015406673774123192\n",
            "step : 14.58 % , loss : 0.02044212631881237\n",
            "step : 15.62 % , loss : 0.009596948511898518\n",
            "step : 16.67 % , loss : 0.016525456681847572\n",
            "step : 17.71 % , loss : 0.016296004876494408\n",
            "step : 18.75 % , loss : 0.012142407707870007\n",
            "step : 19.79 % , loss : 0.008910599164664745\n",
            "step : 20.83 % , loss : 0.005820173770189285\n",
            "step : 21.88 % , loss : 0.005054086912423372\n",
            "step : 22.92 % , loss : 0.007753452751785517\n",
            "step : 23.96 % , loss : 0.010032436810433865\n",
            "step : 25.0 % , loss : 0.010436182841658592\n",
            "step : 26.04 % , loss : 0.011171386577188969\n",
            "step : 27.08 % , loss : 0.004926535300910473\n",
            "step : 28.12 % , loss : 0.004654135089367628\n",
            "step : 29.17 % , loss : 0.006204436998814344\n",
            "step : 30.21 % , loss : 0.0077345846220850945\n",
            "step : 31.25 % , loss : 0.006961530074477196\n",
            "step : 32.29 % , loss : 0.010320877656340599\n",
            "step : 33.33 % , loss : 0.005520565900951624\n",
            "step : 34.38 % , loss : 0.014291968196630478\n",
            "step : 35.42 % , loss : 0.007643712218850851\n",
            "step : 36.46 % , loss : 0.004730702843517065\n",
            "step : 37.5 % , loss : 0.008999296464025974\n",
            "step : 38.54 % , loss : 0.0059955306351184845\n",
            "step : 39.58 % , loss : 0.01576826348900795\n",
            "step : 40.62 % , loss : 0.012306847609579563\n",
            "step : 41.67 % , loss : 0.005050936713814735\n",
            "step : 42.71 % , loss : 0.01528804749250412\n",
            "step : 43.75 % , loss : 0.0035653961822390556\n",
            "step : 44.79 % , loss : 0.003911125939339399\n",
            "step : 45.83 % , loss : 0.007740693166851997\n",
            "step : 46.88 % , loss : 0.007688988000154495\n",
            "step : 47.92 % , loss : 0.005090938415378332\n",
            "step : 48.96 % , loss : 0.007160469423979521\n",
            "step : 50.0 % , loss : 0.009566247463226318\n",
            "step : 51.04 % , loss : 0.011794846504926682\n",
            "step : 52.08 % , loss : 0.0081045962870121\n",
            "step : 53.12 % , loss : 0.006482377648353577\n",
            "step : 54.17 % , loss : 0.008186250925064087\n",
            "step : 55.21 % , loss : 0.01578618586063385\n",
            "step : 56.25 % , loss : 0.00955959502607584\n",
            "step : 57.29 % , loss : 0.019983554258942604\n",
            "step : 58.33 % , loss : 0.006833185441792011\n",
            "step : 59.38 % , loss : 0.006142845377326012\n",
            "step : 60.42 % , loss : 0.003789250273257494\n",
            "step : 61.46 % , loss : 0.008731619454920292\n",
            "step : 62.5 % , loss : 0.006480999756604433\n",
            "step : 63.54 % , loss : 0.007863238453865051\n",
            "step : 64.58 % , loss : 0.002711305860430002\n",
            "step : 65.62 % , loss : 0.003414237406104803\n",
            "step : 66.67 % , loss : 0.005438930355012417\n",
            "step : 67.71 % , loss : 0.006530457641929388\n",
            "step : 68.75 % , loss : 0.013402707874774933\n",
            "step : 69.79 % , loss : 0.0037153593730181456\n",
            "step : 70.83 % , loss : 0.008205682970583439\n",
            "step : 71.88 % , loss : 0.012404737062752247\n",
            "step : 72.92 % , loss : 0.004109533503651619\n",
            "step : 73.96 % , loss : 0.007616919931024313\n",
            "step : 75.0 % , loss : 0.008051583543419838\n",
            "step : 76.04 % , loss : 0.00784683134406805\n",
            "step : 77.08 % , loss : 0.004679784644395113\n",
            "step : 78.12 % , loss : 0.006675171200186014\n",
            "step : 79.17 % , loss : 0.007276596967130899\n",
            "step : 80.21 % , loss : 0.008561920374631882\n",
            "step : 81.25 % , loss : 0.0067598773166537285\n",
            "step : 82.29 % , loss : 0.005145605653524399\n",
            "step : 83.33 % , loss : 0.009501139633357525\n",
            "step : 84.38 % , loss : 0.0040453518740832806\n",
            "step : 85.42 % , loss : 0.009954120963811874\n",
            "step : 86.46 % , loss : 0.01247075293213129\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-e2cc12325eba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-93683f2b9df1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(total_epoch, best_loss)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-93683f2b9df1>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, clip)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_reshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "run(total_epoch=epoch, best_loss=inf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4hQMqlqKVyM"
      },
      "source": [
        "## Training"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.7.9 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e4b32415eeaa5c3d86513c3ab4aafabd9c64e823c547e8bb9b074fa1a9fe4a90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
