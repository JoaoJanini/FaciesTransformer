{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heOekHXdFLvS"
      },
      "source": [
        "## Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNPKl6i8FJes",
        "outputId": "740d8ec0-66e4-4464-a0a1-57807aa47046"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ai0mx9U9Zc"
      },
      "source": [
        "## Import repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEmL3M0EkbRn",
        "outputId": "540dc099-0eb4-40ae-94d1-92a90d59eaa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n",
            "Cloning into 'learning'...\n",
            "remote: Enumerating objects: 223, done.\u001b[K\n",
            "remote: Counting objects: 100% (223/223), done.\u001b[K\n",
            "remote: Compressing objects: 100% (146/146), done.\u001b[K\n",
            "remote: Total 223 (delta 108), reused 170 (delta 67), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (223/223), 241.15 KiB | 6.35 MiB/s, done.\n",
            "Resolving deltas: 100% (108/108), done.\n"
          ]
        }
      ],
      "source": [
        "%rm -rf /content/seq2seq\n",
        "import getpass\n",
        "!git clone --branch feature/use_proper_classes_embedding https://{getpass.getpass()}@github.com/JoaoJanini/seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1-8t1kQmMby"
      },
      "outputs": [],
      "source": [
        "!sleep 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHjmuJYhWoI5",
        "outputId": "9c39ffc2-6270-4791-9073-1fef23585244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/learning\n"
          ]
        }
      ],
      "source": [
        "%cd seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlH_RmEXhuUm"
      },
      "outputs": [],
      "source": [
        "%mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIAA6KzFWMpq",
        "outputId": "5b7d9eb7-e2ff-4867-f71b-7bb15f8bdb1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (2.9.1)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (0.13.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (0.0)\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.0.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (3.17.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (0.6.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (0.37.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (3.4.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (1.50.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 5)) (0.4.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (2022.9.24)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (0.2.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->-r requirements.txt (line 5)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->-r requirements.txt (line 5)) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 5)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->-r requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2022.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn->-r requirements.txt (line 9)) (1.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics->-r requirements.txt (line 10)) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics->-r requirements.txt (line 10)) (3.0.9)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn->-r requirements.txt (line 9)) (1.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGHFjYn7W-Sp"
      },
      "source": [
        "# Sequence to Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvf1FcV4jC63"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "path = \"./seq2seq\"\n",
        "if path not in sys.path:\n",
        "  sys.path.append(path)\n",
        "else:\n",
        "  print(path + \"already in path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhe1k3RvjYlO",
        "outputId": "96cbc7e0-7861-4bd7-824f-56df1fb7405b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " './seq2seq']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "957DWfDKe61U"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "model_directory = f\"/content/drive/MyDrive/Coding/seq2seq/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA1Fy5OZ1oT1"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odiTVvrE1qMt"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, logging\n",
        "from hf_sequence_to_sequence.model import FaciesForConditionalGeneration\n",
        "from hf_sequence_to_sequence.configuration import FaciesConfig\n",
        "import torchmetrics\n",
        "import math\n",
        "import time\n",
        "from torch import nn, optim\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset.dataset import WellsDataset\n",
        "from torch.utils.data import random_split\n",
        "from typing import List\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "from utils import compute_metrics_fn, collate_fn, ray_hp_space\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "\n",
        "# define function to compute metrics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 256\n",
        "SEQUENCE_LEN = 15\n",
        "TRAINING_RATIO = 0.95\n",
        "WIRELINE_LOGS_HEADER = [\"GR\", \"NPHI\", \"RSHA\", \"DTC\", \"RHOB\", \"SP\"]\n",
        "LABEL_COLUMN_HEADER = [\"FORCE_2020_LITHOFACIES_LITHOLOGY\"]\n",
        "\n",
        "train_dataset = WellsDataset(\n",
        "    dataset_type=\"train\",\n",
        "    sequence_len=SEQUENCE_LEN,\n",
        "    model_type=\"seq2seq\",\n",
        "    feature_columns=WIRELINE_LOGS_HEADER,\n",
        "    label_columns=LABEL_COLUMN_HEADER,\n",
        ")\n",
        "\n",
        "DATA_LEN = train_dataset.train_len\n",
        "d_input = train_dataset.input_len\n",
        "d_output = train_dataset.output_len\n",
        "d_channel = train_dataset.channel_len\n",
        "tgt_vocab_size = train_dataset.output_len + len(train_dataset.special_symbols)\n",
        "TRAIN_DATA_LEN = int(DATA_LEN * TRAINING_RATIO)\n",
        "\n",
        "train_data, validation_data = random_split(\n",
        "    train_dataset, lengths=[TRAIN_DATA_LEN, DATA_LEN - TRAIN_DATA_LEN]\n",
        ")\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "\n",
        "\n",
        "facies_config = {\n",
        "    \"vocab_size\": tgt_vocab_size,\n",
        "    \"max_position_embeddings\": 1024,\n",
        "    \"encoder_layers\": 6,\n",
        "    \"encoder_ffn_dim\": 1024,\n",
        "    \"encoder_attention_heads\": 8,\n",
        "    \"decoder_layers\": 4,\n",
        "    \"decoder_ffn_dim\": 1024,\n",
        "    \"decoder_attention_heads\": 8,\n",
        "    \"encoder_layerdrop\": 0.0,\n",
        "    \"decoder_layerdrop\": 0.0,\n",
        "    \"activation_function\": \"relu\",\n",
        "    \"d_model\": 512,\n",
        "    \"n_input_features\": d_channel,\n",
        "    \"n_output_features\": d_output,\n",
        "    \"sequence_len\": SEQUENCE_LEN,\n",
        "    \"dropout\": 0.2,\n",
        "    \"attention_dropout\": 0.0,\n",
        "    \"activation_dropout\": 0.0,\n",
        "    \"init_std\": 0.02,\n",
        "    \"classifier_dropout\": 0.0,\n",
        "    \"scale_embedding\": False,\n",
        "    \"use_cache\": False,\n",
        "    \"num_labels\": tgt_vocab_size,\n",
        "    \"pad_token_id\": train_dataset.PAD_IDX,\n",
        "    \"bos_token_id\": train_dataset.PAD_IDX,\n",
        "    \"eos_token_id\": train_dataset.PAD_IDX,\n",
        "    \"is_encoder_decoder\": True,\n",
        "    \"decoder_start_token_id\": train_dataset.PAD_IDX,\n",
        "    \"forced_eos_token_id\": train_dataset.PAD_IDX,\n",
        "}\n",
        "facies_transformer_config = FaciesConfig(**facies_config)\n",
        "facies_transformer_config.save_pretrained(\n",
        "    f\"{model_directory}/facies-transformer-config\"\n",
        ")\n",
        "facies_transformer_config = FaciesConfig.from_pretrained(\n",
        "    f\"{model_directory}/facies-transformer-config\"\n",
        ")\n",
        "\n",
        "\n",
        "def model_init(trial):\n",
        "\n",
        "    return FaciesForConditionalGeneration(facies_transformer_config)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=f\"{model_directory}/facies-transformer\",\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    disable_tqdm=True\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=None,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=validation_data,\n",
        "    data_collator=collate_fn,\n",
        "    args=training_args,\n",
        "    model_init=model_init,\n",
        "    compute_metrics=compute_metrics_fn,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_model = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    n_trials=10,\n",
        "    search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\n",
        "    hp_space=ray_hp_space, \n",
        "    local_dir=f\"{model_directory}/ray_results\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S501D4Xk1rSh",
        "outputId": "55b75a54-6423-4f97-cdca-8f8ed6b87ed4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "data structure: [lines, timesteps, features]\n",
            "train data size: [(76492, 10, 2)]\n",
            "Number of classes: 12\n"
          ]
        }
      ],
      "source": [
        "test_dataset = WellsDataset(\n",
        "    dataset_type=\"test\",\n",
        "    sequence_len=SEQUENCE_LEN,\n",
        "    model_type=\"seq2seq\",\n",
        "    feature_columns=WIRELINE_LOGS_HEADER,\n",
        "    label_columns=LABEL_COLUMN_HEADER,\n",
        "    scaler=train_dataset.scaler,\n",
        "    output_len=train_dataset.output_len,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "torch.save(\n",
        "    best_model.state_dict(),\n",
        "    f=f\"{model_directory}/facies-transformer/facies_transformer_state_dict.pt\",\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "99c6707e6b461d11f09cb1797f7e40511c3362fe9c917a4510d9e5853ba54b32"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
