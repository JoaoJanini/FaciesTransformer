{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoJanini/seq2seq/blob/fix%2Fswitch_to_seq_to_seq_trainer/notebooks/seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heOekHXdFLvS"
      },
      "source": [
        "## Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNPKl6i8FJes",
        "outputId": "e4dbe05f-37eb-4e8d-b2d3-a3fa3eea40d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7ai0mx9U9Zc"
      },
      "source": [
        "## Import repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YEmL3M0EkbRn",
        "outputId": "be9716cd-08ab-4d44-91f4-f8d5b4a8aee6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "··········\n",
            "Cloning into 'seq2seq'...\n",
            "remote: Enumerating objects: 289, done.\u001b[K\n",
            "remote: Counting objects: 100% (289/289), done.\u001b[K\n",
            "remote: Compressing objects: 100% (173/173), done.\u001b[K\n",
            "remote: Total 289 (delta 154), reused 220 (delta 92), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (289/289), 1.38 MiB | 4.45 MiB/s, done.\n",
            "Resolving deltas: 100% (154/154), done.\n"
          ]
        }
      ],
      "source": [
        "%rm -rf /content/seq2seq\n",
        "import getpass\n",
        "!git clone --branch fix/switch_to_seq_to_seq_trainer https://{getpass.getpass()}@github.com/JoaoJanini/seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "U1-8t1kQmMby"
      },
      "outputs": [],
      "source": [
        "!sleep 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHjmuJYhWoI5",
        "outputId": "eae31e43-1389-4b86-8595-de92428a378a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/seq2seq\n"
          ]
        }
      ],
      "source": [
        "%cd seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tlH_RmEXhuUm"
      },
      "outputs": [],
      "source": [
        "%mkdir data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIAA6KzFWMpq",
        "outputId": "412e3a11-620b-4919-ef24-362a69261fb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torch>=1.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.13.1+cu113)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.21.6)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (0.11.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (2.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (1.3.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (1.0.2)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.10.3-py3-none-any.whl (529 kB)\n",
            "\u001b[K     |████████████████████████████████| 529 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5 MB 82.7 MB/s \n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.14.0-py3-none-any.whl (175 kB)\n",
            "\u001b[K     |████████████████████████████████| 175 kB 17.3 MB/s \n",
            "\u001b[?25hCollecting ray[tune]\n",
            "  Downloading ray-2.1.0-cp37-cp37m-manylinux2014_x86_64.whl (59.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 59.1 MB 276 kB/s \n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 80.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 15)) (1.7.3)\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 86.5 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-profile\n",
            "  Downloading tensorboard_plugin_profile-2.8.0-py3-none-any.whl (5.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.3 MB 72.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchsummary in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 18)) (1.5.1)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 19)) (0.1.2)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1->-r requirements.txt (line 1)) (4.1.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (57.4.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (1.8.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (0.38.4)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (2.14.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (3.19.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.14->-r requirements.txt (line 6)) (1.50.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 6)) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 6)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 6)) (4.9)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 6)) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->-r requirements.txt (line 6)) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=1.14->-r requirements.txt (line 6)) (3.10.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=1.14->-r requirements.txt (line 6)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->-r requirements.txt (line 6)) (3.2.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn->-r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 7)) (2022.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->-r requirements.txt (line 9)) (1.2.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics->-r requirements.txt (line 10)) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 11)) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 11)) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 72.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 11)) (3.8.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 73.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from accelerate->-r requirements.txt (line 12)) (5.4.8)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 14)) (3.8.3)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 14)) (2022.11.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 14)) (9.0.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 14)) (0.3.6)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 34.5 MB/s \n",
            "\u001b[?25hCollecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 69.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (22.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (1.8.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (6.0.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (1.3.3)\n",
            "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (0.13.0)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets->-r requirements.txt (line 14)) (2.1.1)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 70.1 MB/s \n",
            "\u001b[?25hCollecting gviz-api>=1.9.0\n",
            "  Downloading gviz_api-1.10.0-py2.py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt->-r requirements.txt (line 19)) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt->-r requirements.txt (line 19)) (4.3.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt->-r requirements.txt (line 19)) (0.16.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pymongo->hyperopt->-r requirements.txt (line 19)) (2.2.1)\n",
            "Collecting virtualenv>=20.0.24\n",
            "  Downloading virtualenv-20.17.0-py3-none-any.whl (8.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.8 MB 58.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[tune]->-r requirements.txt (line 13)) (4.3.3)\n",
            "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]->-r requirements.txt (line 13)) (7.1.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[tune]->-r requirements.txt (line 13)) (1.0.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[tune]->-r requirements.txt (line 13)) (0.8.10)\n",
            "Collecting distlib<1,>=0.3.6\n",
            "  Downloading distlib-0.3.6-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[K     |████████████████████████████████| 468 kB 68.8 MB/s \n",
            "\u001b[?25hCollecting platformdirs<3,>=2.4\n",
            "  Downloading platformdirs-2.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]->-r requirements.txt (line 13)) (0.19.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[tune]->-r requirements.txt (line 13)) (5.10.0)\n",
            "Installing collected packages: urllib3, platformdirs, distlib, xxhash, virtualenv, responses, multiprocess, huggingface-hub, tokenizers, tensorboardX, ray, gviz-api, datasets, transformers, torchmetrics, tensorboard-plugin-profile, evaluate, accelerate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed accelerate-0.14.0 datasets-2.7.1 distlib-0.3.6 evaluate-0.3.0 gviz-api-1.10.0 huggingface-hub-0.11.1 multiprocess-0.70.14 platformdirs-2.5.4 ray-2.1.0 responses-0.18.0 tensorboard-plugin-profile-2.8.0 tensorboardX-2.5.1 tokenizers-0.13.2 torchmetrics-0.10.3 transformers-4.24.0 urllib3-1.25.11 virtualenv-20.17.0 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -r requirements.txt "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGHFjYn7W-Sp"
      },
      "source": [
        "# Sequence to Sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Xvf1FcV4jC63"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "path = \"./seq2seq\"\n",
        "if path not in sys.path:\n",
        "  sys.path.append(path)\n",
        "else:\n",
        "  print(path + \"already in path\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhe1k3RvjYlO",
        "outputId": "6c93dda3-fe87-488d-f9f4-7b5eaf45a966"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content',\n",
              " '/env/python',\n",
              " '/usr/lib/python37.zip',\n",
              " '/usr/lib/python3.7',\n",
              " '/usr/lib/python3.7/lib-dynload',\n",
              " '',\n",
              " '/usr/local/lib/python3.7/dist-packages',\n",
              " '/usr/lib/python3/dist-packages',\n",
              " '/usr/local/lib/python3.7/dist-packages/IPython/extensions',\n",
              " '/root/.ipython',\n",
              " './seq2seq']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sys.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "957DWfDKe61U"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer, logging, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from hf_sequence_to_sequence.model import FaciesForConditionalGeneration\n",
        "from hf_sequence_to_sequence.configuration import FaciesConfig\n",
        "import torchmetrics\n",
        "import math\n",
        "import time\n",
        "from torch import nn, optim\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from dataset.dataset import WellsDataset\n",
        "from torch.utils.data import random_split\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_dataset, load_metric\n",
        "from utils import collate_fn\n",
        "from ray.tune.schedulers import PopulationBasedTraining\n",
        "from ray.tune.search.hyperopt import HyperOptSearch\n",
        "from ray import tune\n",
        "# define function to compute metrics\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "EVAL_BATCH_SIZE = 640\n",
        "SEQUENCE_LEN = 15\n",
        "TRAINING_RATIO = 0.90\n",
        "WIRELINE_LOGS_HEADER = [\"GR\", \"NPHI\", \"RSHA\", \"DTC\", \"RHOB\", \"SP\"]\n",
        "LABEL_COLUMN_HEADER = [\"FORCE_2020_LITHOFACIES_LITHOLOGY\"]\n",
        "model_directory = f\"/content/drive/MyDrive/Coding/seq2seq/{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
        "\n",
        "train_dataset = WellsDataset(\n",
        "    dataset_type=\"train\",\n",
        "    sequence_len=SEQUENCE_LEN,\n",
        "    model_type=\"seq2seq\",\n",
        "    feature_columns=WIRELINE_LOGS_HEADER,\n",
        "    label_columns=LABEL_COLUMN_HEADER,\n",
        ")\n",
        "test_dataset = WellsDataset(\n",
        "    dataset_type=\"test\",\n",
        "    sequence_len=SEQUENCE_LEN,\n",
        "    model_type=\"seq2seq\",\n",
        "    feature_columns=WIRELINE_LOGS_HEADER,\n",
        "    label_columns=LABEL_COLUMN_HEADER,\n",
        "    scaler=train_dataset.scaler,\n",
        "    output_len=train_dataset.output_len,\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "DATA_LEN = train_dataset.train_len\n",
        "d_input = train_dataset.input_len\n",
        "d_output = train_dataset.output_len\n",
        "d_channel = train_dataset.channel_len\n",
        "tgt_vocab_size = train_dataset.output_len + len(train_dataset.special_symbols)\n",
        "TRAIN_DATA_LEN = int(DATA_LEN * TRAINING_RATIO)\n",
        "\n",
        "train_data, validation_data = random_split(\n",
        "    train_dataset, lengths=[TRAIN_DATA_LEN, DATA_LEN - TRAIN_DATA_LEN]\n",
        ")\n",
        "\n",
        "\n",
        "facies_config = {\n",
        "    \"vocab_size\": tgt_vocab_size,\n",
        "    \"max_position_embeddings\": 1024,\n",
        "    \"encoder_layers\": 6,\n",
        "    \"encoder_ffn_dim\": 1024,\n",
        "    \"encoder_attention_heads\": 8,\n",
        "    \"decoder_layers\": 6,\n",
        "    \"decoder_ffn_dim\": 1024,\n",
        "    \"decoder_attention_heads\": 8,\n",
        "    \"encoder_layerdrop\": 0.0,\n",
        "    \"decoder_layerdrop\": 0.0,\n",
        "    \"activation_function\": \"relu\",\n",
        "    \"d_model\": 1024,\n",
        "    \"n_input_features\": d_input,\n",
        "    \"n_output_features\": d_output,\n",
        "    \"sequence_len\": SEQUENCE_LEN,\n",
        "    \"dropout\": 0.2,\n",
        "    \"attention_dropout\": 0.0,\n",
        "    \"activation_dropout\": 0.0,\n",
        "    \"init_std\": 0.02,\n",
        "    \"classifier_dropout\": 0.0,\n",
        "    \"scale_embedding\": False,\n",
        "    \"use_cache\": False,\n",
        "    \"num_labels\": tgt_vocab_size,\n",
        "    \"pad_token_id\": train_dataset.PAD_IDX,\n",
        "    \"bos_token_id\": train_dataset.PAD_IDX,\n",
        "    \"eos_token_id\": train_dataset.PAD_IDX,\n",
        "    \"is_encoder_decoder\": True,\n",
        "    \"decoder_start_token_id\": train_dataset.PAD_IDX,\n",
        "    \"forced_eos_token_id\": train_dataset.PAD_IDX,\n",
        "    \"return_dict\": False,\n",
        "}\n",
        "facies_transformer_config = FaciesConfig(**facies_config)\n",
        "facies_transformer_config.save_pretrained(\n",
        "    f\"{model_directory}/facies-transformer-config\"\n",
        ")\n",
        "facies_transformer_config = FaciesConfig.from_pretrained(\n",
        "    f\"{model_directory}/facies-transformer-config\"\n",
        ")\n",
        "\n",
        "\n",
        "# function to collate data samples into batch tesors\n",
        "def ray_hp_space(trial):\n",
        "    return {\n",
        "        \"learning_rate\": tune.loguniform(1e-6, 1e-4),\n",
        "        \"per_device_train_batch_size\": tune.choice([16, 640, 1280]),\n",
        "        \"weight_decay\": tune.uniform(0.0, 0.3),\n",
        "        \"num_train_epochs\": tune.choice([5, 10, 15]),\n",
        "        \"dropout\": tune.choice([0.3, 0.1, 0.2, 0.0, 0.4]),\n",
        "    }\n",
        "\n",
        "def compute_metrics_fn(eval_preds):\n",
        "    metrics = dict()\n",
        "    accuracy_metric = load_metric(\"accuracy\")\n",
        "    preds = eval_preds.predictions[:, 1:-1]\n",
        "    preds = preds.flatten()\n",
        "    labels = eval_preds.label_ids[:,:-2]\n",
        "    labels = labels.flatten()\n",
        "    preds = preds[labels != 0]\n",
        "    labels = labels[labels != 0]\n",
        "\n",
        "    metrics.update(accuracy_metric.compute(predictions=preds, references=labels))\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def model_init(trial):\n",
        "    model_config = facies_transformer_config\n",
        "    if trial is not None:\n",
        "        model_config.update({\"dropout\": trial[\"dropout\"]})\n",
        "    print(\"model_init() called. updated config is\")\n",
        "    print(model_config)\n",
        "\n",
        "    return FaciesForConditionalGeneration(model_config)\n",
        "\n",
        "    return FaciesForConditionalGeneration(facies_transformer_config)\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=f\"{model_directory}/facies-transformer\",\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=10,\n",
        "    generation_max_length=SEQUENCE_LEN + 2,\n",
        "    generation_num_beams=4,\n",
        "    predict_with_generate=True\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=None,\n",
        "    train_dataset=train_data,\n",
        "    data_collator=collate_fn,\n",
        "    eval_dataset=validation_data,\n",
        "    args=training_args,\n",
        "    model_init = model_init,\n",
        "    compute_metrics=compute_metrics_fn\n",
        ")\n",
        "\n",
        "best_model = trainer.hyperparameter_search(\n",
        "    direction=\"maximize\",\n",
        "    backend=\"ray\",\n",
        "    n_trials=10,\n",
        "    search_alg=HyperOptSearch(metric=\"objective\", mode=\"max\"),\n",
        "    hp_space=ray_hp_space,\n",
        "    local_dir=f\"{model_directory}/ray_results\",\n",
        "    resources_per_trial={\"gpu\": 1, \"cpu\":1}\n",
        ")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "99c6707e6b461d11f09cb1797f7e40511c3362fe9c917a4510d9e5853ba54b32"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}